{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Toolkits\\anaconda2-4.2.0\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "# Imports\n",
    "import os\n",
    "import scipy.io as sio\n",
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# import required libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Dropout, Input, BatchNormalization, Conv1D\n",
    "from keras.layers.core import Activation\n",
    "from keras import metrics, regularizers\n",
    "from keras.callbacks import TensorBoard\n",
    "import pickle\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import RidgeClassifierCV,LogisticRegressionCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sys.path.append(\"../\") # go to parent dir\n",
    "from mrcode.utils.file_utils import fileFinder, folderFinder\n",
    "from mrcode import settings\n",
    "\n",
    "data_path, log_path, experiment_data_path = settings.path_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/logs//test1/'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path1 = log_path + '/test1/'\n",
    "data_path1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def std_windows(eeg, time_window = 50):\n",
    "    temp_eeg = []\n",
    "    for ii in range(eeg.shape[0]):\n",
    "        temp_epoch = eeg[ii,:,:]\n",
    "        temp_eeg.append(np.std(temp_epoch.reshape(int(temp_epoch.shape[0]/time_window),time_window,temp_epoch.shape[1]),1).flatten())\n",
    "    std_eeg = np.array(temp_eeg)\n",
    "    return std_eeg\n",
    "\n",
    "\n",
    "def std_overlap_windows(eeg, time_window = 80, overlap = 0.5):\n",
    "    temp_eeg = []\n",
    "    for ii in range(eeg.shape[0]):\n",
    "        temp_epoch = eeg[ii,:,:]\n",
    "        std_channels = []\n",
    "        for jj in range(temp_epoch.shape[1]):\n",
    "            channel = temp_epoch[:,jj]\n",
    "\n",
    "            std_channel = []\n",
    "            for kk in range(int(time_window*overlap)):\n",
    "                std_channel.append(channel[kk:(kk+time_window)])\n",
    "\n",
    "            std_channels.append(np.std(std_channel,1))\n",
    "\n",
    "        temp_eeg.append(std_channels)\n",
    "        \n",
    "    temp_eeg_array = np.array(temp_eeg)\n",
    "    std_temp_eeg = temp_eeg_array.reshape(temp_eeg_array.shape[0],temp_eeg_array.shape[2]*temp_eeg_array.shape[1])\n",
    "    return std_temp_eeg\n",
    "\n",
    "\n",
    "def load_data(folders,feature = 'raw'):\n",
    "    for count, ii in enumerate(folders):\n",
    "        data = sio.loadmat(ii + '/eeg_events.mat')\n",
    "        eeg = data['eeg_events'].transpose()\n",
    "\n",
    "        if feature == 'raw':\n",
    "            eeg = eeg.reshape(eeg.shape[0],eeg.shape[2]*eeg.shape[1])\n",
    "\n",
    "        if feature == 'std':\n",
    "            eeg = std_windows(eeg)\n",
    "\n",
    "        if feature == 'std_overlap':\n",
    "            eeg = std_overlap_windows(eeg)\n",
    "\n",
    "        image_info = pd.read_csv(ii + '/image_order.txt', delimiter='\\t')\n",
    "        yield count, eeg, image_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_generator(folders):\n",
    "    for ii in folders:\n",
    "        eeg, image_info = load_data(ii,data_path, feature = loadmode)\n",
    "    yield eeg, image_info\n",
    "\n",
    "\n",
    "data_path = '../data/experiment_data'\n",
    "experiment_folders = [data_path + '/' + ii for ii in folderFinder(data_path)]\n",
    "X_train_temp = []\n",
    "y_train_temp = []\n",
    "target = 'category'\n",
    "loadmode = 'raw'\n",
    "\n",
    "for count, eeg, image_info in load_data(experiment_folders):   \n",
    "    if count == 0:\n",
    "        X_train_temp = eeg\n",
    "        y_train_temp = list(image_info[target].as_matrix())\n",
    "    \n",
    "    elif count > 0:\n",
    "        X_train_temp = np.vstack((X_train_temp,eeg))\n",
    "        y_train_temp = np.hstack((y_train_temp,list(image_info[target].as_matrix())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/experiment_data/exp1',\n",
       " '../data/experiment_data/exp10',\n",
       " '../data/experiment_data/exp11',\n",
       " '../data/experiment_data/exp12',\n",
       " '../data/experiment_data/exp13',\n",
       " '../data/experiment_data/exp14',\n",
       " '../data/experiment_data/exp15',\n",
       " '../data/experiment_data/exp2',\n",
       " '../data/experiment_data/exp3',\n",
       " '../data/experiment_data/exp4',\n",
       " '../data/experiment_data/exp5',\n",
       " '../data/experiment_data/exp6',\n",
       " '../data/experiment_data/exp7',\n",
       " '../data/experiment_data/exp8',\n",
       " '../data/experiment_data/exp9']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_folders = [data_path + '/' + ii for ii in folderFinder(data_path)]\n",
    "experiment_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10005, 17600)\n",
      "(10005, 23)\n",
      "(345, 17600)\n",
      "(345, 23)\n"
     ]
    }
   ],
   "source": [
    "data_path = '../data/experiment_data'\n",
    "experiment_folders = folderFinder(data_path)\n",
    "X_train_temp = []\n",
    "y_train_temp = []\n",
    "target = 'category'\n",
    "loadmode = 'raw'\n",
    "for count_sources, ii in enumerate(experiment_folders):\n",
    "    \n",
    "    eeg, image_info = load_data(ii,data_path, feature = loadmode)\n",
    "    \n",
    "    if count_sources == 0:\n",
    "        X_train_temp = eeg\n",
    "        y_train_temp = list(image_info[target].as_matrix())\n",
    "    \n",
    "    elif count_sources > 0:\n",
    "        X_train_temp = np.vstack((X_train_temp,eeg))\n",
    "        y_train_temp = np.hstack((y_train_temp,list(image_info[target].as_matrix())))\n",
    "    \n",
    "b = list(range(20,690*len(experiment_folders),30))\n",
    "y = pd.get_dummies(y_train_temp)\n",
    "\n",
    "\n",
    "# Split data \n",
    "y_train = np.array(y)\n",
    "y_test = y_train[b,:]\n",
    "y_train = np.delete(y_train,b,0)\n",
    "X_test = X_train_temp[b,:]\n",
    "X_train = np.delete(X_train_temp,b,0)\n",
    "\n",
    "\n",
    "# Normalize data\n",
    "xScale = StandardScaler().fit(X_train)\n",
    "X_train = xScale.transform(X_train)\n",
    "X_test = xScale.transform(X_test)\n",
    "\n",
    "# For CNN training\n",
    "#X_train = X_train.reshape((X_train.shape[0], 1,X_train.shape[1]))\n",
    "#X_test = X_test.reshape((X_test.shape[0], 1,X_test.shape[1]))\n",
    "#y_train = y_train.reshape((y_train.shape[0], 1, y_train.shape[1]))\n",
    "#y_test = y_test.reshape((y_test.shape[0], 1,y_test.shape[1]))\n",
    "# Printing shape of data\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(690, 17600)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eeg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = list(range(20,690*len(experiment_folders),30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# enabling SMOTE sampling\n",
    "to_catogorical_dict = dict(enumerate(y.columns))\n",
    "y_categorical = np.array([to_catogorical_dict[ii] for ii in y_train_temp])\n",
    "y_train_categorical = np.delete(y_categorical,b,0)\n",
    "y_test_categorical = y_categorical[b]\n",
    "\n",
    "X_train_padding = np.zeros((30*(len(experiment_folders)),X_train_before_padding.shape[1]))\n",
    "y_train_padding = np.ones((30*(len(experiment_folders))),int)*len(y.columns)\n",
    "X_with_padding = np.vstack([X_train_before_padding,X_train_padding])\n",
    "y_with_padding = np.concatenate([y_train_categorical,y_train_padding])\n",
    "\n",
    "# Use SMOTE to generate new samples equal to the amount used in the test set\n",
    "sm = SMOTE(ratio='all', random_state=1)\n",
    "X_res, y_res = sm.fit_sample(X_with_padding, y_with_padding)\n",
    "\n",
    "# Remove padding from training set\n",
    "padding_to_remove = np.where(~X_res.any(axis=1))[0]\n",
    "X_train = np.delete(X_res,padding_to_remove,0)\n",
    "y_train_categorical = np.delete(y_res,padding_to_remove,0)\n",
    "\n",
    "flippedDict = {y:x for x,y in to_catogorical_dict.items()}\n",
    "y_train_temp2 = np.array([flippedDict[ii] for ii in y_train_categorical])\n",
    "y2 = pd.get_dummies(y_train_temp2)\n",
    "y_train = np.array(y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the percentage of each classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airplane      0.043478\n",
       "bear          0.043478\n",
       "bed           0.043478\n",
       "bench         0.043478\n",
       "bird          0.043478\n",
       "boat          0.043478\n",
       "bus           0.043478\n",
       "cat           0.043478\n",
       "clock         0.043478\n",
       "cow           0.043478\n",
       "dog           0.043478\n",
       "donut         0.043478\n",
       "elephant      0.043478\n",
       "giraffe       0.043478\n",
       "horse         0.043478\n",
       "motorcycle    0.043478\n",
       "pizza         0.043478\n",
       "sheep         0.043478\n",
       "stop sign     0.043478\n",
       "teddy bear    0.043478\n",
       "toilet        0.043478\n",
       "train         0.043478\n",
       "zebra         0.043478\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.sum()/sum(y.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale the data and generate new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10350, 1, 17600)\n",
      "(10350,)\n",
      "(345, 1, 17600)\n",
      "(345, 23)\n"
     ]
    }
   ],
   "source": [
    "# Split data \n",
    "X_test = X_train_temp[b,:]\n",
    "X_train_before_padding = np.delete(X_train_temp,b,0)\n",
    "X_train_padding = np.zeros((30*(len(experiment_folders)),X_train_before_padding.shape[1]))\n",
    "y_train_padding = np.ones((30*(len(experiment_folders))),int)*len(y.columns)\n",
    "X_with_padding = np.vstack([X_train_before_padding,X_train_padding])\n",
    "y_with_padding = np.concatenate([y_train_categorical,y_train_padding])\n",
    "\n",
    "# Use SMOTE to generate new samples equal to the amount used in the test set\n",
    "sm = SMOTE(ratio='all', random_state=1)\n",
    "X_res, y_res = sm.fit_sample(X_with_padding, y_with_padding)\n",
    "\n",
    "# Remove padding from training set\n",
    "padding_to_remove = np.where(~X_res.any(axis=1))[0]\n",
    "X_train = np.delete(X_res,padding_to_remove,0)\n",
    "y_train_categorical = np.delete(y_res,padding_to_remove,0)\n",
    "\n",
    "flippedDict = {y:x for x,y in to_catogorical_dict.items()}\n",
    "y_train_temp2 = np.array([flippedDict[ii] for ii in y_train_categorical])\n",
    "y2 = pd.get_dummies(y_train_temp2)\n",
    "y_train = np.array(y2)\n",
    "y_train = y_train.reshape((y_train.shape[0], 1, y_train.shape[1]))\n",
    "y_test = y_test.reshape((y_test.shape[0], 1,y_test.shape[1]))\n",
    "\n",
    "# Normalize data\n",
    "xScale = StandardScaler().fit(X_train)\n",
    "X_train = xScale.transform(X_train)\n",
    "X_train = X_train.reshape((X_train.shape[0], 1,X_train.shape[1]))\n",
    "X_test = xScale.transform(X_test)\n",
    "X_test = X_test.reshape((X_test.shape[0], 1,X_test.shape[1]))\n",
    "# Printing shape of data\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_categorical = np.delete(y_res,padding_to_remove,0)\n",
    "\n",
    "flippedDict = {y:x for x,y in to_catogorical_dict.items()}\n",
    "y_train_temp2 = np.array([flippedDict[ii] for ii in y_train_categorical])\n",
    "y2 = pd.get_dummies(y_train_temp2)\n",
    "y_train = np.array(y2)\n",
    "y_train = y_train.reshape((y_train.shape[0], 1, y_train.shape[1]))\n",
    "y_test = y_test.reshape((y_test.shape[0], 1,y_test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "\n",
    "weight_regularizer=regularizers.l2(0.00)\n",
    "\n",
    "keras_optimizer = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "#keras_optimizer = optimizers.SGD(lr=0.01, momentum=0.1, decay=0.0, nesterov=True)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(8,25, name='conv1',padding='same', \n",
    "                kernel_initializer='he_normal', kernel_regularizer=weight_regularizer, \n",
    "                input_shape=(None,X_train.shape[2])))\n",
    "\n",
    "#model.add(Dropout(0.4, name='first_dropout'))\n",
    "model.add(BatchNormalization(name='first_batch_norm'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "model.add(Conv1D(32,25, name='conv2', padding='same',\n",
    "                kernel_initializer='he_normal', kernel_regularizer=weight_regularizer))\n",
    "#model.add(Dropout(0.4))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(y_train.shape[2], name='conv3', \n",
    "                kernel_initializer='he_normal', kernel_regularizer=weight_regularizer))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softmax', name='Softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=keras_optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1 (Conv1D)               (None, None, 8)           3520008   \n",
      "_________________________________________________________________\n",
      "first_batch_norm (BatchNorma (None, None, 8)           32        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, None, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv1D)               (None, None, 32)          6432      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, None, 32)          128       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, None, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv3 (Dense)                (None, None, 23)          759       \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, None, 23)          92        \n",
      "_________________________________________________________________\n",
      "Softmax (Activation)         (None, None, 23)          0         \n",
      "=================================================================\n",
      "Total params: 3,527,451\n",
      "Trainable params: 3,527,325\n",
      "Non-trainable params: 126\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Toolkits\\anaconda2-4.2.0\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "Train on 10350 samples, validate on 345 samples\n",
      "Epoch 1/20\n",
      "10350/10350 [==============================] - 61s 6ms/step - loss: 3.3782 - acc: 0.0460 - val_loss: 3.2092 - val_acc: 0.0667\n",
      "Epoch 2/20\n",
      "10350/10350 [==============================] - 54s 5ms/step - loss: 3.2675 - acc: 0.0542 - val_loss: 3.1753 - val_acc: 0.0667\n",
      "Epoch 3/20\n",
      "10350/10350 [==============================] - 53s 5ms/step - loss: 3.2139 - acc: 0.0595 - val_loss: 3.1405 - val_acc: 0.0928\n",
      "Epoch 4/20\n",
      "10350/10350 [==============================] - 54s 5ms/step - loss: 3.1772 - acc: 0.0678 - val_loss: 3.1416 - val_acc: 0.0812\n",
      "Epoch 5/20\n",
      "10350/10350 [==============================] - 56s 5ms/step - loss: 3.1363 - acc: 0.0751 - val_loss: 3.1398 - val_acc: 0.0667\n",
      "Epoch 6/20\n",
      "10350/10350 [==============================] - 56s 5ms/step - loss: 3.1028 - acc: 0.0788 - val_loss: 3.1337 - val_acc: 0.0667\n",
      "Epoch 7/20\n",
      "10350/10350 [==============================] - 55s 5ms/step - loss: 3.0691 - acc: 0.0861 - val_loss: 3.1196 - val_acc: 0.0609\n",
      "Epoch 8/20\n",
      "10350/10350 [==============================] - 54s 5ms/step - loss: 3.0421 - acc: 0.1005 - val_loss: 3.0888 - val_acc: 0.0899\n",
      "Epoch 9/20\n",
      "10350/10350 [==============================] - 53s 5ms/step - loss: 3.0099 - acc: 0.1085 - val_loss: 3.0795 - val_acc: 0.0812\n",
      "Epoch 10/20\n",
      "10350/10350 [==============================] - 55s 5ms/step - loss: 2.9880 - acc: 0.1172 - val_loss: 3.1071 - val_acc: 0.0638\n",
      "Epoch 11/20\n",
      "10350/10350 [==============================] - 53s 5ms/step - loss: 2.9713 - acc: 0.1186 - val_loss: 3.1080 - val_acc: 0.0754\n",
      "Epoch 12/20\n",
      "10350/10350 [==============================] - 53s 5ms/step - loss: 2.9589 - acc: 0.1196 - val_loss: 3.0959 - val_acc: 0.0783\n",
      "Epoch 13/20\n",
      "10350/10350 [==============================] - 53s 5ms/step - loss: 2.9388 - acc: 0.1273 - val_loss: 3.0893 - val_acc: 0.0812\n",
      "Epoch 14/20\n",
      "10350/10350 [==============================] - 53s 5ms/step - loss: 2.9242 - acc: 0.1300 - val_loss: 3.1197 - val_acc: 0.0754\n",
      "Epoch 15/20\n",
      "10350/10350 [==============================] - 55s 5ms/step - loss: 2.9139 - acc: 0.1368 - val_loss: 3.0974 - val_acc: 0.0638\n",
      "Epoch 16/20\n",
      "10350/10350 [==============================] - 53s 5ms/step - loss: 2.8878 - acc: 0.1456 - val_loss: 3.1284 - val_acc: 0.0812\n",
      "Epoch 17/20\n",
      "10350/10350 [==============================] - 53s 5ms/step - loss: 2.8712 - acc: 0.1470 - val_loss: 3.1451 - val_acc: 0.0696\n",
      "Epoch 18/20\n",
      "10350/10350 [==============================] - 57s 5ms/step - loss: 2.8546 - acc: 0.1524 - val_loss: 3.1402 - val_acc: 0.0928\n",
      "Epoch 19/20\n",
      "10350/10350 [==============================] - 54s 5ms/step - loss: 2.8359 - acc: 0.1586 - val_loss: 3.1678 - val_acc: 0.0696\n",
      "Epoch 20/20\n",
      "10350/10350 [==============================] - 57s 5ms/step - loss: 2.8183 - acc: 0.1659 - val_loss: 3.1999 - val_acc: 0.0696\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 20\n",
    "\n",
    "class TrainValTensorBoard(TensorBoard):\n",
    "    def __init__(self, log_dir='../data/logs', **kwargs):\n",
    "        # Make the original `TensorBoard` log to a subdirectory 'training'\n",
    "        training_log_dir = os.path.join(log_dir, 'training')\n",
    "        super(TrainValTensorBoard, self).__init__(training_log_dir, **kwargs)\n",
    "\n",
    "        # Log the validation metrics to a separate subdirectory\n",
    "        self.val_log_dir = os.path.join(log_dir, 'validation')\n",
    "\n",
    "    def set_model(self, model):\n",
    "        # Setup writer for validation metrics\n",
    "        self.val_writer = tf.summary.FileWriter(self.val_log_dir)\n",
    "        super(TrainValTensorBoard, self).set_model(model)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Pop the validation logs and handle them separately with\n",
    "        # `self.val_writer`. Also rename the keys so that they can\n",
    "        # be plotted on the same figure with the training metrics\n",
    "        logs = logs or {}\n",
    "        val_logs = {k.replace('val_', ''): v for k, v in logs.items() if k.startswith('val_')}\n",
    "        for name, value in val_logs.items():\n",
    "            summary = tf.Summary()\n",
    "            summary_value = summary.value.add()\n",
    "            summary_value.simple_value = value.item()\n",
    "            summary_value.tag = name\n",
    "            self.val_writer.add_summary(summary, epoch)\n",
    "        self.val_writer.flush()\n",
    "\n",
    "        # Pass the remaining logs to `TensorBoard.on_epoch_end`\n",
    "        logs = {k: v for k, v in logs.items() if not k.startswith('val_')}\n",
    "        super(TrainValTensorBoard, self).on_epoch_end(epoch, logs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        super(TrainValTensorBoard, self).on_train_end(logs)\n",
    "\"\"\"        \n",
    "tbCallBack = TrainValTensorBoard(log_dir= data_path1, histogram_freq=2,\n",
    "                         batch_size=batch_size, write_graph=False,\n",
    "                         write_grads=True, write_images=True,\n",
    "                         embeddings_freq=0, embeddings_layer_names=None,\n",
    "                         embeddings_metadata=None)\n",
    "\"\"\"\n",
    "\n",
    "tbCallBack = TrainValTensorBoard(log_dir= data_path1, histogram_freq=1,\n",
    "                         batch_size=batch_size, write_graph=False,\n",
    "                         write_grads=True, write_images=True,\n",
    "                         embeddings_freq=0, embeddings_layer_names=None,\n",
    "                         embeddings_metadata=None)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "                    shuffle=True, validation_data=(X_test, y_test),\n",
    "                    callbacks=[tbCallBack])\n",
    "\n",
    "# For tensorboard viz\n",
    "# http://desktop-0c9v0dk:6006\n",
    "# tensorboard --logdir=C:\\Users\\nicol\\Documents\\GitHub\\Project-MindReading\\data\\logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hej = model.get_updates_for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_new = y_train.argmax(1)\n",
    "y_test_new = y_test.argmax(1)\n",
    "y_train_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ridge_model = LogisticRegressionCV(cv =3)\n",
    "ridge_model.fit(X_train,y_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_predict = ridge_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum(y_predict-y_test_new==0)/len(y_test_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction keras method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testPredict = model.predict(X_test)\n",
    "predict_classes = testPredict.argmax(1)\n",
    "true_classes = y_test.argmax(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(true_classes)\n",
    "#print(predict_classes)\n",
    "\n",
    "hej = np.array(predict_classes-true_classes == 0)\n",
    "hej = sum(hej.reshape(5, 23),0)\n",
    "print(hej)\n",
    "print(sum(hej)/115)\n",
    "#len(predict_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = metrics.top_k_categorical_accuracy(y_test,testPredict,k=1)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "v = sess.run(predictions)    \n",
    "print(v) # will show you your variable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
